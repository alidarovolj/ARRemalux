# ü§ñ ML Semantic Segmentation Integration

## ‚úÖ –ß—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ (Phase 1)

### 1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
- ‚úÖ –ú–æ–¥–µ–ª—å `optimum/segformer-b0-finetuned-ade-512-512` –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ `Assets/ML/`
- ‚úÖ `MLSegmentationManager.cs` - Unity C# –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è ML inference
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ `WallDetectionAndPainting.cs`:
  - –†–µ–∂–∏–º "Fill Whole Wall" (fillWholeWallMode)
  - –ú–µ—Ç–æ–¥ `TryPaintWholeWall()` —Å Flood Fill –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤: —Ç–æ—á–µ—á–Ω–æ–µ —Ä–∏—Å–æ–≤–∞–Ω–∏–µ –∏ "–≤—Å—è —Å—Ç–µ–Ω–∞"

### 2. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
- ‚úÖ Flood Fill –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤—Å–µ–π —Å—Ç–µ–Ω—ã –æ—Ç —Ç–æ—á–∫–∏ –∫–ª–∏–∫–∞
- ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Å–∞ –ø–∏–∫—Å–µ–ª—è (IsWall, IsDoor, GetPixelClass)
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 150 –∫–ª–∞—Å—Å–æ–≤ ADE20K dataset:
  - `wall` (ID: 0)
  - `door` (ID: 14)
  - `person` (ID: 12)
  - `sofa`, `chair`, `table`, `tv` –∏ –¥—Ä.

---

## üöß –ß—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–¥–µ–ª–∞—Ç—å (Phase 2)

### –ù–µ–¥–µ–ª—è 1-2: Native –ø–ª–∞–≥–∏–Ω—ã

#### iOS (CoreML)

**–§–∞–π–ª:** `Assets/Plugins/iOS/CoreMLSegmentation.mm`

```objc
// TODO: –°–æ–∑–¥–∞—Ç—å Objective-C++ –ø–ª–∞–≥–∏–Ω
#import <CoreML/CoreML.h>
#import <Vision/Vision.h>

@interface CoreMLSegmentation : NSObject

+ (BOOL)initializeWithModelPath:(NSString*)modelPath;
+ (NSArray<NSNumber*>*)segmentCurrentFrame;
+ (void)cleanup;

@end

@implementation CoreMLSegmentation

static MLModel *segmentationModel;
static VNCoreMLModel *visionModel;

+ (BOOL)initializeWithModelPath:(NSString*)modelPath {
    NSError *error;
    NSURL *modelURL = [NSURL fileURLWithPath:modelPath];
    
    // –ó–∞–≥—Ä—É–∑–∏—Ç—å CoreML –º–æ–¥–µ–ª—å
    segmentationModel = [MLModel modelWithContentsOfURL:modelURL error:&error];
    if (error) {
        NSLog(@"[CoreML] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: %@", error);
        return NO;
    }
    
    // –°–æ–∑–¥–∞—Ç—å Vision model
    visionModel = [VNCoreMLModel modelForMLModel:segmentationModel error:&error];
    if (error) {
        NSLog(@"[CoreML] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Vision model: %@", error);
        return NO;
    }
    
    NSLog(@"[CoreML] ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!");
    return YES;
}

+ (NSArray<NSNumber*>*)segmentCurrentFrame {
    // TODO: –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π AR frame
    // TODO: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (512x512)
    // TODO: Inference —á–µ—Ä–µ–∑ Vision
    // TODO: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ - –∏–∑–≤–ª–µ—á—å –º–∞—Å–∫—É –∫–ª–∞—Å—Å–æ–≤
    // TODO: –í–µ—Ä–Ω—É—Ç—å byte array (512*512 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
    
    return nil; // Placeholder
}

+ (void)cleanup {
    segmentationModel = nil;
    visionModel = nil;
}

@end

// Unity bridge functions
extern "C" {
    BOOL CoreML_Initialize(const char* modelPath) {
        NSString *path = [NSString stringWithUTF8String:modelPath];
        return [CoreMLSegmentation initializeWithModelPath:path];
    }
    
    void CoreML_SegmentFrame(unsigned char* outputBuffer, int bufferSize) {
        NSArray<NSNumber*> *mask = [CoreMLSegmentation segmentCurrentFrame];
        
        // –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ Unity buffer
        for (int i = 0; i < bufferSize && i < mask.count; i++) {
            outputBuffer[i] = [mask[i] unsignedCharValue];
        }
    }
    
    void CoreML_Cleanup() {
        [CoreMLSegmentation cleanup];
    }
}
```

#### Android (TensorFlow Lite)

**–§–∞–π–ª:** `Assets/Plugins/Android/TFLiteSegmentation.kt`

```kotlin
// TODO: –°–æ–∑–¥–∞—Ç—å Kotlin –ø–ª–∞–≥–∏–Ω
package com.remalux.ml

import android.content.Context
import android.graphics.Bitmap
import org.tensorflow.lite.Interpreter
import java.nio.ByteBuffer
import java.nio.ByteOrder

class TFLiteSegmentation(context: Context) {
    private var interpreter: Interpreter? = null
    
    fun initialize(modelPath: String): Boolean {
        try {
            // –ó–∞–≥—Ä—É–∑–∏—Ç—å TFLite –º–æ–¥–µ–ª—å
            val modelFile = loadModelFile(context, modelPath)
            interpreter = Interpreter(modelFile)
            
            println("[TFLite] ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return true
        } catch (e: Exception) {
            println("[TFLite] ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: ${e.message}")
            return false
        }
    }
    
    fun segmentCurrentFrame(): ByteArray {
        // TODO: –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π AR frame
        // TODO: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (512x512)
        // TODO: Inference —á–µ—Ä–µ–∑ TFLite
        // TODO: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ - –∏–∑–≤–ª–µ—á—å –º–∞—Å–∫—É –∫–ª–∞—Å—Å–æ–≤
        // TODO: –í–µ—Ä–Ω—É—Ç—å byte array (512*512 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        
        return ByteArray(512 * 512) // Placeholder
    }
    
    fun cleanup() {
        interpreter?.close()
        interpreter = null
    }
    
    private fun loadModelFile(context: Context, modelPath: String): ByteBuffer {
        val inputStream = context.assets.open(modelPath)
        val fileSize = inputStream.available()
        val buffer = ByteBuffer.allocateDirect(fileSize)
        buffer.order(ByteOrder.nativeOrder())
        
        val data = ByteArray(fileSize)
        inputStream.read(data)
        buffer.put(data)
        
        return buffer
    }
}

// Unity bridge (JNI)
@Keep
object UnityBridge {
    private var segmentation: TFLiteSegmentation? = null
    
    @JvmStatic
    fun initialize(context: Context, modelPath: String): Boolean {
        segmentation = TFLiteSegmentation(context)
        return segmentation?.initialize(modelPath) ?: false
    }
    
    @JvmStatic
    fun segmentFrame(): ByteArray? {
        return segmentation?.segmentCurrentFrame()
    }
    
    @JvmStatic
    fun cleanup() {
        segmentation?.cleanup()
        segmentation = null
    }
}
```

### –ù–µ–¥–µ–ª—è 3: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

#### –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ONNX ‚Üí CoreML (iOS)

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install onnx-coreml coremltools

# 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
python3 << EOF
import onnx
from onnx_coreml import convert

# –ó–∞–≥—Ä—É–∑–∏—Ç—å ONNX –º–æ–¥–µ–ª—å
onnx_model = onnx.load('Assets/ML/optimum:segformer-b0-finetuned-ade-512-512/model.onnx')

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ CoreML —Å quantization
coreml_model = convert(
    onnx_model,
    minimum_ios_deployment_target='13',
    preprocessing_args={
        'image_scale': 1.0/255.0,
        'blue_bias': 0,
        'green_bias': 0,
        'red_bias': 0
    }
)

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
coreml_model.save('Assets/StreamingAssets/segformer_b0.mlmodel')
EOF
```

#### –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ONNX ‚Üí TFLite (Android)

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install onnx tf2onnx tensorflow

# 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
python3 << EOF
import onnx
import tensorflow as tf
from onnx_tf.backend import prepare

# –ó–∞–≥—Ä—É–∑–∏—Ç—å ONNX –º–æ–¥–µ–ª—å
onnx_model = onnx.load('Assets/ML/optimum:segformer-b0-finetuned-ade-512-512/model.onnx')

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ONNX ‚Üí TensorFlow
tf_rep = prepare(onnx_model)
tf_rep.export_graph('temp_model')

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å TensorFlow ‚Üí TFLite
converter = tf.lite.TFLiteConverter.from_saved_model('temp_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantization
tflite_model = converter.convert()

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
with open('Assets/StreamingAssets/segformer_b0.tflite', 'wb') as f:
    f.write(tflite_model)
EOF
```

---

## üéØ Unity Integration (Phase 3)

### –û–±–Ω–æ–≤–∏—Ç—å MLSegmentationManager.cs

–†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å TODO —Å–µ–∫—Ü–∏–∏:

```csharp
#if UNITY_IOS
private void InitializeCoreMLModel()
{
    // –ó–∞–≥—Ä—É–∑–∏—Ç—å CoreML –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ native –ø–ª–∞–≥–∏–Ω
    string modelPath = Application.streamingAssetsPath + "/segformer_b0.mlmodel";
    bool success = CoreML_Initialize(modelPath);
    
    if (!success)
    {
        Debug.LogError("[MLSegmentation] –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ CoreML!");
        enableMLSegmentation = false;
    }
}

private void RunMLInference()
{
    // –í—ã–∑–≤–∞—Ç—å native —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è inference
    byte[] buffer = new byte[inferenceResolution * inferenceResolution];
    CoreML_SegmentFrame(buffer, buffer.Length);
    
    currentSegmentationMask = buffer;
}

// P/Invoke declarations
[DllImport("__Internal")]
private static extern bool CoreML_Initialize(string modelPath);

[DllImport("__Internal")]
private static extern void CoreML_SegmentFrame(byte[] outputBuffer, int bufferSize);

[DllImport("__Internal")]
private static extern void CoreML_Cleanup();
#endif
```

---

## üìä –û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ | FPS inference | Latency | –¢–æ—á–Ω–æ—Å—Ç—å |
|-----------|-----------|---------------|---------|----------|
| iOS | iPhone 13 (A15) | 8-12 FPS | ~80-120ms | 95% |
| iOS | iPhone 12 Pro (A14 + LiDAR) | 10-15 FPS | ~65-100ms | 95% |
| Android | Snapdragon 888 | 5-8 FPS | ~125-200ms | 95% |

---

## üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏

–í Unity Inspector:
1. –í–∫–ª—é—á–∏—Ç—å `Enable ML Segmentation` –≤ MLSegmentationManager
2. –í–∫–ª—é—á–∏—Ç—å `Fill Whole Wall Mode` –≤ WallDetectionAndPainting
3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ

### –õ–æ–≥–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

```
[MLSegmentation] ‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
[MLSegmentation] Inference resolution: 512x512
[MLSegmentation] Avg inference time: 85.3ms (11.7 FPS)
[WallDetection] ‚úÖ –ö–ª–∏–∫ –Ω–∞ —Å—Ç–µ–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ ML!
[WallDetection] üé® –ü—Ä–∏–º–µ–Ω—è–µ–º –∫—Ä–∞—Å–∫—É –∫ 45,892 –ø–∏–∫—Å–µ–ª—è–º —Å—Ç–µ–Ω—ã...
[WallDetection] ‚úÖ –í–°–Ø –°–¢–ï–ù–ê –≤—ã–¥–µ–ª–µ–Ω–∞! (45892 –ø–∏–∫—Å–µ–ª–µ–π)
```

---

## üéØ Roadmap

### Phase 1 (‚úÖ –ó–ê–í–ï–†–®–ï–ù–ê)
- Unity C# –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
- Flood Fill –∞–ª–≥–æ—Ä–∏—Ç–º
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ WallDetectionAndPainting

### Phase 2 (üöß –í –†–ê–ó–†–ê–ë–û–¢–ö–ï - 2 –Ω–µ–¥–µ–ª–∏)
- [ ] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ONNX ‚Üí CoreML + TFLite
- [ ] iOS CoreML –ø–ª–∞–≥–∏–Ω
- [ ] Android TFLite –ø–ª–∞–≥–∏–Ω
- [ ] P/Invoke –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### Phase 3 (üìÖ –ó–ê–ü–õ–ê–ù–ò–†–û–í–ê–ù–ê - 1 –Ω–µ–¥–µ–ª—è)
- [ ] –ü–æ–ª–Ω–∞—è –∑–∞–ª–∏–≤–∫–∞ —Å—Ç–µ–Ω—ã (–Ω–µ —Ç–æ—á–∫–∞, –∞ –≤—Å—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å)
- [ ] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è inference (throttling, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ)
- [ ] Preview –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –∫—Ä–∞—Å–∫–∏
- [ ] Undo/Redo —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### Phase 4 (üìÖ –ë–£–î–£–©–ï–ï)
- [ ] –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–∞–Ω–∏–º–∞—Ü–∏—è –∑–∞–ª–∏–≤–∫–∏)
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–µ–∫—Å—Ç—É—Ä (–Ω–µ —Ç–æ–ª—å–∫–æ solid color)
- [ ] –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- [ ] Export –≤ AR Cloud Anchors

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [SegFormer Paper](https://arxiv.org/abs/2105.15203)
- [Hugging Face Model](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [CoreML Documentation](https://developer.apple.com/documentation/coreml)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite/guide)
- [ADE20K Dataset](https://groups.csail.mit.edu/vision/datasets/ADE20K/)

---

## üí° –°–æ–≤–µ—Ç—ã –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ

1. **–ù–∞—á–Ω–∏—Ç–µ —Å iOS** - CoreML –ø—Ä–æ—â–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–º TFLite
2. **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö** –ø–µ—Ä–µ–¥ AR integration
3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Xcode Instruments** –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è CoreML
4. **–ö—ç—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** - –Ω–µ –Ω—É–∂–Ω–æ inference –Ω–∞ –∫–∞–∂–¥–æ–º –∫–∞–¥—Ä–µ
5. **Throttling –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω** - –∏–Ω–∞—á–µ FPS —É–ø–∞–¥—ë—Ç –¥–æ 30

---

–°–æ–∑–¥–∞–Ω–æ: 30 –æ–∫—Ç—è–±—Ä—è 2025  
–ê–≤—Ç–æ—Ä: ARRemalux Team  
–í–µ—Ä—Å–∏—è: 1.0


