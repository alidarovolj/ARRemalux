Инструкция для Cursor AI (Claude 4.5) по дальнейшему развитию AR-проекта
Обзор проекта и цели

Этот проект представляет собой кросс-платформенное AR-приложение (iOS и Android), предназначенное для сканирования окружающего пространства и виртуальной покраски/рисования на реальных поверхностях в дополненной реальности. Иными словами, пользователь с помощью камеры устройства может сканировать помещение (получать 3D-меш окружающей сцены) и рисовать на видимых поверхностях (например, стены, пол, объекты) виртуальной кистью.

Текущий статус: На данный момент проделана определённая базовая работа:

Настроены AR-сессии на iOS (с использованием ARKit) и Android (с использованием ARCore) для отслеживания окружающей среды и получения плоскостей/глубины.

Архитектура проекта определена предварительно, приложение разделено на модули: AR-модуль (работа с камерой, сенсорами и отображением контента) и модуль логики (управление данными сцены, пользовательским вводом и т.д.).

UI/UX: реализовано базовое отображение камеры и поверх неё – рендеринг виртуальных объектов. Например, возможно, уже выводится сетка точек или плоскостей, обнаруженных AR-системой.

Дополнительно: Обсуждалась возможность применения моделей машинного зрения (например, сегментации изображений) для улучшения понимания сцены, однако полноценная интеграция ML-модели пока не выполнена.

Основная цель дальнейшей работы – реализовать полнофункциональное сканирование пространства и рисование, оптимизируя при этом архитектуру и производительность. Необходимо обеспечить, чтобы на iOS и Android приложение функционировало аналогично, несмотря на различия между ARKit и ARCore. Также, при необходимости, следует интегрировать алгоритмы компьютерного зрения (OpenCV, SegFormer или другие ML-модели) для семантического понимания сцены – если это обоснованно для достижения целей приложения.

В ходе дальнейшей разработки требуется:

Завершить реализацию AR-мешинга (построения 3D-модели окружения) на iOS, и подобрать аналогичный подход для Android (с учётом ограничений ARCore).

Реализовать интерактивное рисование/покраску в AR – пользователь должен иметь возможность “рисовать” на реальных поверхностях через камеру (например, оставлять цветные метки на стенах, полу и прочих обнаруженных поверхностях).

При необходимости – интегрировать модели машинного обучения (например, модель сегментации SegFormer) для распознавания объектов или классификации поверхностей в реальном времени, чтобы улучшить качество покраски или ограничить рисование определёнными областями.

Провести анализ текущей архитектуры приложения и оптимизировать её: убедиться, что решение масштабируется на обе платформы, поддерживает высокую производительность (реальное время), а кодовая база остаётся чистой и модульной.

Продолжить реализацию функционала на основе улучшенной архитектуры – т.е. дописать недостающий код, исправить найденные недостатки, провести оптимизации (например, по использованию многопоточности для ML или рендеринга).

Ниже подробно описаны ключевые направления работы, с пояснениями, ссылками на исследования и документацию, а также план дальнейших действий.

1. Реализация AR-мешинга (сканирование окружения)

Задача: научить приложение сканировать геометрию реального пространства и получать трёхмерную сетку (mesh), представляющую поверхности вокруг пользователя. Эта сетка затем может быть использована для визуализации (например, отображать сетку поверх реальных объектов) и для определения, где можно рисовать.

1.1 ARKit (iOS) – сканирование с помощью LiDAR

На iOS следует использовать возможности ARKit для Scene Reconstruction (реконструкции сцены) с помощью LiDAR-сканера (доступно на устройствах iPhone/iPad с LiDAR). ARKit 3.5+ поддерживает генерацию mesh-сетки окружения на LiDAR-устройствах
github.com
docs.unity3d.com
. Ключевые моменты:

ARKit предоставляет типы ARMeshAnchor и ARMeshGeometry, которые содержат вершины, нормали и грани (треугольники) сетки, сканируемой с камеры устройства. Каждые три вершины образуют треугольную грань, и таким образом постепенно строится полигональная модель окружающих поверхностей.

Начиная с iOS 13.4 и ARKit 4, появилась возможность получать не только геометрию, но и семантическую классификацию для каждой грани меша. Каждая грань помечается как пол, стена, потолок, стол, сиденье, окно, дверь или «неизвестно»
docs.unity3d.com
docs.unity3d.com
. Эти классификации доступны через свойство classification у ARMeshAnchor. Таким образом, приложение может различать тип поверхности. В примере от Apple ARKit указано, что “каждый треугольник сетки идентифицирован как один из нескольких типов поверхности”, и демонстрируется раскрашивание разных типов в разные цвета
docs.unity3d.com
.

Визуализация сетки: RealityKit/SceneKit позволяет отобразить полученный mesh. Например, можно создавать ARMeshAnchor-привязанные объекты. В публичном примере проекта Travis Hall показано, как извлекать геометрию из ARMeshAnchor и строить собственный MeshResource в RealityKit, раскрашивая грани по их классификации
github.com
. Каждому типу поверхности (пол, стена, стол и т.д.) можно назначить свой цвет или материал в демонстрационных целях.

Ограничения: полное сканирование окружающего пространства доступно только на устройствах с LiDAR. Требования: iOS 13.4 или новее, устройство с LiDAR (iPad Pro 2020+, iPhone 12 Pro и новее)
docs.unity3d.com
. На устройствах без LiDAR ARKit не строит произвольную mesh-сетку, хотя всё ещё может определять плоскости (пол, стол и т.п.) с помощью области ARPlaneAnchor и делать People Occlusion (сегментацию людей). Поэтому приложение должно предусмотреть проверку возможности Scene Reconstruction (через supportsSceneReconstruction) и реагировать, если функция недоступна (например, сообщать, что сканирование 3D недоступно, или переключаться в режим рисования только на плоскостях).

Практические шаги для iOS:

Включить в ARWorldTrackingConfiguration опцию sceneReconstruction = .mesh (для RealityKit) или соответствующий флаг в ARSession (для ARKit/SceneKit)
github.com
.

Обрабатывать обновления anchors: в session(_:didAdd:) и didUpdate получать новые ARMeshAnchor и добавлять/обновлять их геометрию в сцене.

Хранение сетки: Продумать структуру данных для хранения совокупной сетки. ARKit будет выдавать кусочки меша (анкеры привязаны к определенным областям). Можно объединять их в один общий объект или хранить раздельно по анкерам.

Визуализировать сетку для отладки: можно настроить отображение сетки как полупрозрачную цветную модель. Например, отрисовывать каждую грань с небольшим смещением от реальной поверхности и заливать цветом в зависимости от типа (как в примере classification meshes
docs.unity3d.com
).

1.2 ARCore (Android) – подходы к реконструкции сцены

На Android ситуация сложнее, так как ARCore пока не предоставляет встроенного средства для полного meshing-а окружения (нет аналога ARMeshAnchor на ноябрь 2025). ARCore умеет обнаруживать плоскости (Plane detection) и получать карту глубины (Depth API), но полигональную сетку всей комнаты автоматически он не строит. В официальном форуме Unity отмечалось: «Google не поддерживает построение меша в ARCore, поэтому AR Foundation не может предоставить meshing на ARCore до тех пор, пока Google не реализует эту возможность»
docs.unity3d.com
. Поэтому, для Android платформы доступны такие варианты:

Обнаружение основных плоскостей: ARCore Plane API предоставляет горизонтальные и вертикальные плоскости (стены, полы, столы) посредством ARPlane. Можно ограничиться ими для размещения рисуемых объектов. Например, если цель рисования – стены и пол, то AR-плоскости уже достаточны для привязки рисунка. Однако мелкие детали поверхности не распознаются.

Использование Depth API для частичного meshing-а: ARCore Depth API даёт возможность получать карту глубины – по сути, 2D изображение, где каждому пикселю соответствует расстояние до камеры. С помощью Raw Depth API можно получить необработанную карту глубины и перевести её в облако 3D-точек
codelabs.developers.google.com
. На основе облака точек в теории можно пытаться строить локальные меши. Например, Google Codelab демонстрирует приложение, которое:

Берёт сырую карту глубин с каждой камеры-frames,

Репроецирует её в облако 3D-точек,

Фильтрует точки по достоверности и близости,

Сегментирует скопления точек, представляющие объекты
codelabs.developers.google.com
.

Хотя это не даёт сразу готового меша всех поверхностей, можно по точкам приблизительно восстановить форму окружения. Например, точечное представление стены можно конвертировать в плоскость или грубую сетку (через алгоритмы вроде Poisson surface reconstruction или делауни триангуляции по точкам). Важно: такая реконструкция будет ограниченно точной и требовательной к ресурсам, но для грубой сетки или для определения рельефа поверхности (например, выступающие объекты) может подойти.

Streetscape Geometry API (внешний источник): В 2023 Google представил Streetscape Geometry API, который способен выдавать готовый 3D-меш ближайших зданий и ландшафта в радиусе ~100м для уличных сценариев
developers.googleblog.com
. Однако, это относится только к наружним сценам (outdoor) – по сути, используется геометрия из картографических данных для дополненной реальности. Для внутренних помещений подобного API нет.

Scene Semantics API: ARCore 2023 также получил семантический API для наружных сцен – он выдаёт сегментированное по классам изображение (не меш, а семантика по пикселям) для улиц: небо, здание, дерево, дорога, человек и т.п. 
developers.google.com
developers.google.com
. Но этот API работает только на улице и только в портретной ориентации, для помещений он не предназначен
developers.google.com
. Таким образом, для нашей задачи (рисование внутри комнаты) он непригоден. Мы упоминаем его лишь для полноты, т.к. он демонстрирует, что ARCore включает ML-модель в реальном времени для понимания сцены – возможно, со временем появится и indoor-аналоги.

Практические шаги для Android:

Обнаружение плоскостей: Продолжить использовать стандартный ARCore PlaneDetection. Убедиться, что приложение умеет получать события о новой найденной плоскости (Plane.TYPE_HORIZONTAL или VERTICAL) и обновлять их габариты. На основе плоскостей можно ограничить области рисования (например, рисовать только на стенах и полах, что, возможно, проще и достаточно).

Использование Depth API: Включить в сессии ARCore получение глубины (SessionConfig.DepthMode.AUTOMATIC). Это позволит ARCore автоматически выдавать буфер глубины каждого кадра. С помощью этого можно реализовать окклюзию (виртуальные краски будут перекрыты реальными объектами при необходимости) и также анализировать расстояние до поверхности, когда пользователь рисует.

Например, при тапе по экрану или перетаскивании пальцем мы можем брать не только результат хит-теста по плоскости, но и выборку из карты глубины, чтобы определить рельеф – например, если на стене выступает объект, карта глубины это покажет. Можно предотвратить “рисование сквозь объект”.

Кастомный меш из глубины (опционально): Если решим строить mesh, можно каждые несколько кадров накапливать точки из Depth API (например, преобразуя depth map в точки камеры, а их – в мировые координаты) и пытаться строить локальные треугольники. Это сложная задача, поэтому, возможно, на данном этапе достаточно облака точек: его можно визуализировать для эффекта “сканера” или просто использовать для определения формы.

Google Codelab по Raw Depth (см. ссылку) даёт представление, как можно сегментировать объекты по облаку точек
codelabs.developers.google.com
 – в нашем случае, например, можно пытаться отделять стену от передних объектов.

Ограничение по устройствам: глубина ARCore работает на ограниченном списке девайсов (более новых моделях с поддержкой Depth API). Надо обработать случаи, если Depth недоступен (приложение должно не пытаться активировать соответствующие функции).

Вывод по meshing: Полноценный meshing будет реализован на iOS (при наличии LiDAR), что даст богатую полигональную модель помещения. На Android же придётся ограничиться либо плоскостями (и этого может быть достаточно для покраски стен/пола), либо использовать глубину для приближенного восстановления или хотя бы для расширенного взаимодействия (окклюзия, определение контуров объектов). В дальнейшем, если Google добавит официальную поддержку реконструкции сетки, можно будет обновить Android-реализацию
docs.unity3d.com
.

Важно также унифицировать отображение: к примеру, на iOS можно отобразить сетку реального окружения (для тестирования и “вау-эффекта”), а на Android – отображать обнаруженные плоскости и/или облако точек, чтобы пользователю был виден результат сканирования.

2. AR-рисование на поверхностях (interaction + rendering)

Задача: позволить пользователю рисовать в дополненной реальности – то есть отмечать определённые участки реальных поверхностей виртуальной краской, линиями, рисунками. Это ключевой интерактивный функционал нашего приложения.

Основные требования:

Пользовательский ввод: распознавать жест рисования. Вероятно, самый интуитивный способ на телефоне – касание экрана и движение пальцем, подобно рисованию на экране, но с привязкой к реальному миру. Например, пользователь направляет камеру на стену, прикасается к экрану на изображении стены и “ведёт” пальцем – на той области стены появляется цветная линия в AR.

Привязка к 3D-поверхностям: каждую нарисованную точку/линию нужно привязать к реальной геометрии. Это значит, нужно выполнять hit-test или получить координаты пересечения луча зрения с поверхностью:

На iOS: можно использовать ARView.raycast(...) (RealityKit) или ARSession.raycast (ARKit) с QueryTarget.mesh – это позволит получить координату пересечения луча с mesh-сценой (если LiDAR-меш доступен). Если меш недоступен, можно использовать plane raycast (пересечение с обнаруженными плоскостями).

На Android: использовать frame.hitTest(x, y) с прицелом на Plane – вернётся точка на обнаруженной плоскости. Если хотим рисовать не только на плоскостях, а по произвольной геометрии, можно использовать глубину: получить z из Depth API для указанного пикселя и реконструировать 3D-точку.

Отображение нарисованного: после получения 3D-точки(точек) на поверхности, необходимо создать виртуальный “мазок” в этой точке. Возможные подходы:

Декали/текстуры: Один вариант – проецировать на поверхность текстуру с нарисованным. Например, создать на стене невидимый Mesh (копирующий участок стены) и накладывать на него текстуру, которая обновляется по мере рисования. Это похоже на то, как рисование осуществляется в 2D, но текстура привязана к миру. Apple, например, демонстрировала создание “полотна” в пространстве для рисования жестами
developer.apple.com
. Можно аналогично создать “полотно” вдоль стены (например, плоскость, совпадающую со стеной) и рисовать текстуру.

Частицы/спрайты (рисование точками): Более простой, но менее производительный путь – при каждом перемещении пальца создавать маленький цветной billboard-спрайт (например, кружок или сферу) на поверхности. Многие AR-приложения рисования идут по этому пути: фактически создаётся цепочка маленьких кружков, следующих за пальцем, имитируя линию. Эти кружки размещаются на расстоянии немного перед стеной, чтобы не проваливаться, и всегда повернуты к камере (или лучше совмещены с поверхностью).

Динамический меш “штриха”: Более продвинутый способ – генерировать непосредственно mesh линии. Например, если пользователь ведёт пальцем, брать последовательность 3D-точек и соединять их полигональной трубкой или лентой. RealityKit 2 и далее позволяют создавать динамические mesh через MeshResource.generate(...). WWDC 2024 демонстрирует, как в пространственном рисовании можно генерировать трубчатые кистевые штрихи для плавных линий
developer.apple.com
developer.apple.com
. Этот метод даст более красивый результат (гладкие 3D-линии), но потребует сложной генерации геометрии.

Раскраска самого mesh-а: Если у нас есть полный mesh стены (на LiDAR), теоретически можно изменять цвета вершин/граней этого mesh-а. Например, найти грань, ближайшую к точке касания, и изменить её цвет (или текстуру). Однако, ARKit не предоставляет простой способ перекрашивать mesh-якорь напрямую – придётся копировать его геометрию в свой объект (как сделал Travis Hall для раскраски классификаций
github.com
). Раскрашивание отдельных граней может смотреться грубо (треугольники крупные). Более высокая точность потребует разбиения граней или текстурирования.

Выбор цвета/кисти: Предусмотреть UI для выбора цвета краски, толщины кисти и т.п. (возможно, реализовано частично). Claude может генерировать код UI-элементов (например, кнопки цветов) и их связывание с параметрами рисования.

Учёт окклюзии: В идеале, нарисованные виртуальные мазки должны окклюдироваться реальными объектами корректно. Например, если нарисовали на стене за столом, и пользователь проходит камерой так, что стол закроет рисунок – виртуальная краска не должна “висеть поверх” стола. Для этого нужно использовать Depth buffer occlusion:

На iOS ARKit автоматически может скрывать виртуальные объекты за реальными, если включить опцию кадровой глубины/текстуры сегментации (ARView/ARSCNView has environment.sceneDepth or People Occlusion features). Либо можно рендерить mesh окружения как маску-окклюдер (Unity делает так: строит mesh и использует его в слое глубины
docs.unity3d.com
).

На Android ARCore, если Depth API включён, ARCore XR Plugin в Unity и Sceneform на Android нативно тоже предлагают Environment depth occlusion – т.е. можно настроить, чтобы глубина из камеры применялась к виртуальным объектам
docs.unity3d.com
. В отсутствии готового движка, можно вручную использовать шейдер, который сравнивает пиксельную глубину сцены и глубину виртуального объекта, и скрывает виртуальные фрагменты, когда реальный объект ближе.

Ссылки/примеры:

Apple демонстрировала пример “рисования в пространстве” – приложение, позволяющее пользователю “pinch-to-draw” жестом рисовать линии в воздухе и на поверхностях
developer.apple.com
. Это подтверждает возможность реализовать интерактивное рисование с использованием ARKit.

В докладе WWDC 2024 "Build a spatial drawing app with RealityKit" показано, как объединяются ARKit, RealityKit и SwiftUI для рисования от руки. Используются низкоуровневые Mesh и Texture API для быстрого обновления мазков
developer.apple.com
developer.apple.com
. В нашем случае, нам возможно не нужны такие сложные шейдерные техники, но стоит учесть принцип: для производительности лучше обновлять одну mesh-геометрию, чем создавать тысячи отдельных объектов.

Если рисование реализуется через текстуры, может потребоваться использовать Projective texture painting – существуют исследования, где реальная стена сканируется, создаётся текстура по её площади, и затем кистью можно рисовать прямо на текстуре стены. Это аккуратный способ, но требует UV-развёртки меша стены (можно упростить для плоских поверхностей).

Практические шаги (Claude может их реализовать в коде):

Обработка ввода: Ввести режим рисования – например, при касании экрана получать точку и выполнять raycast. При движении пальца – серию raycast-ов. Для снижения нагрузки, можно отслеживать движение пальца через определённый интервал (например, каждые 50 мс) или при прохождении определённого расстояния, чтобы не создавать избыточно плотные точки.

Создание мазков: В зависимости от выбранного подхода, создать либо:

Новый объект для каждого сегмента линии (например, маленькую сферу/плоскость с материалом заданного цвета), размещённый в той позиции, куда указывает raycast. Объект должен ориентироваться параллельно поверхности (чтобы “наклеиться” на неё). Orientation можно взять из нормали mesh-а (на iOS ARMeshAnchor предоставляет нормали вершин) или из hitResult.getHitPose() на Android.

Или накапливать точки и обновлять одну общую линию. Например, хранить текущий полилинию. Но тогда нужно генерировать геометрию – для начала, Claude может сделать упрощенно: соединять точки цилиндриками или квадратиками.

Оптимизация: Убедиться, что созданные объекты не слишком тяжелы. Если рисование интенсивное, сотни мелких объектов могут снижать производительность. Возможные оптимизации: объединять близкие сегменты в один меш, удалять очень старые или мелкие объекты, ограничивать длину линии.

UI для управления рисованием: Включить возможность выбирать цвет (например, цветовое колесо или набор кнопок) – это влияет на материал создаваемых мазков. Также кнопка “очистить” для удаления всех нарисованных элементов.

Примечание: Чтобы Claude успешно сгенерировал код, нужно будет предоставить контекст фреймворка. Вероятно, на iOS будет удобно использовать RealityKit (Swift) или SceneKit + ARKit. На Android – либо Sceneform (если еще применим), либо прямую работу с ARCore через OpenGL/Filament. Возможно, целесообразно рассмотреть Unity как альтернативу (см. раздел архитектуры), но если остаёмся на нативных платформах, код придётся разделять.

3. Интеграция моделей машинного обучения (OpenCV, SegFormer и др.)

Задача (опциональная): улучшить понимание сцены и поведение рисования с помощью ML. Например:

Семантическая сегментация сцены: определить, где в изображении находятся разные объекты или области (стены, пол, мебель, человек и т.д.), чтобы ограничить рисование только определенными зонами или окрашивать разные поверхности по-разному.

Детекция объектов: обнаружить конкретные объекты, которые не следует закрашивать (например, понимать, где находится мебель, чтобы не “рисовать” сквозь неё, если нет окклюзии).

Повышение точности mesh на Android: с помощью сегментации можно разделять облако точек по объектам, как делается в codelab – что может помочь строить раздельные меши для разных объектов.

Ключевая модель, предложенная для использования – SegFormer. Это современная модель сегментации на базе трансформеров, примечательная тем, что сочетает высокую точность с эффективностью. Исследования NVIDIA показали, что SegFormer достигает 84% mIoU на наборе Cityscapes, при этом самый младший вариант SegFormer-B0 содержит всего ~3.8 млн параметров и способен работать ~48 FPS на изображениях высокого разрешения
labellerr.com
. То есть, SegFormer-B0 достаточно лёгкий для выполнения на мобильных устройствах в режиме реального времени. Технические характеристики: ~3.75M параметров, модель ~14 MB в FP32, которую можно квантовать до ~4 MB
aihub.qualcomm.com
 – вполне подходит для встраивания в приложение.

Подход к интеграции:

На iOS: наиболее рационально конвертировать модель SegFormer (или альтернативную) в Core ML формат. Благодаря Core ML, модель сможет выполняться на Neural Engine или GPU, ускоряя вывод. Apple предоставляет готовые модели сегментации, например, DeepLabv3 в CoreML (упоминалось, что они доступны на Apple Developer сайте
rozengain.medium.com
, также DeepLab упоминается как популярный вариант
it-jim.com
it-jim.com
). Можно либо использовать DeepLab (у которого тоже есть мобильные версии и неплохая скорость
it-jim.com
), либо собственноручно конвертировать SegFormer. Однако, CoreML может не иметь готовой поддержки трансформеров, возможно потребуется немного оптимизировать/пре-тренировать.

На Android: здесь вариантов больше:

TensorFlow Lite (TFLite): конвертировать модель (SegFormer-B0) в TFLite и выполнять с ускорением NNAPI или GPU Delegate. TFLite хорошо подходит для мобильных моделей.

NNAPI напрямую: менее удобно, лучше через TFLite.

OpenCV DNN: как упоминалось пользователем, можно использовать модуль OpenCV DNN, который поддерживает ONNX модели. SegFormer можно экспортировать в ONNX и загрузить OpenCV. Преимущество – код C++/Java будет одинаковым на iOS и Android, используя OpenCV, без необходимости разных фреймворков. Однако, производительность OpenCV DNN может уступать CoreML/NNAPI, т.к. не всегда использует все оптимизации железа.

Модель SegFormer и классы: SegFormer обучен, например, на наборе ADE20K с 150 классами
aihub.qualcomm.com
 (включая множество типов объектов: стены, окна, мебель, люди и т.д.). Нужно решить, какие классы нам важны. Возможно, достаточно крупных категорий: стены, пол, потолок, мебель, человек. Можно даже сгруппировать классы (например, все виды мебели трактовать одинаково). Это уменьшит сложность обработки результатов.

Реальное время vs качество: Чтобы удержать высокое FPS, можно:

Использовать самую маленькую модель (B0).

Уменьшить разрешение входного изображения (например, 256x256 вместо 512x512), пожертвовав точностью границ.

Не запускать сегментацию каждый кадр. Например, выполнять ML-инференс 5-10 раз в секунду, а не 30. Этого достаточно, т.к. сегментация соседних кадров обычно похожа.

Выполнять сегментацию только когда пользователь рисует или для предпросмотра. Возможно, нет смысла всегда держать модель загруженной, если пользователь ничего не делает. Можно загружать модель по требованию (lazy init) – но учесть задержку и память.

Использование результатов: После получения карты сегментации (например, матрицы классов для каждого пикселя):

Можно масштабировать её до размера экрана и применять как маску, запрещая рисование на некоторых областях. Например, если хотим рисовать только на стенах, модель поможет отличить стену от двери, окна, мебели. ARKit хотя и даёт классификацию меша (стена, пол и т.д.), но может не различить объект, висящий на стене. SegFormer же способен отдельно пометить картину или телевизор на стене как object или конкретный класс – мы можем избегать рисования на них.

Можно использовать сегментацию для окраски: например, интересная возможность – раскрасить автоматически разные семантические области разными цветами (как раскраска мира). Но в нашем приложении скорее пользователь сам рисует.

Occlusion (люди): ARKit имеет встроенную People Occlusion – сегментацию людей, скрывающую виртуальные объекты
stanford.edu
. Если мы хотим на Android аналогичное, можно использовать сегментацию, вырезая пиксели класса person из рендеринга (т.е. не рисовать на людях и скрывать мазки за людьми).

Дополнительные эффекты: Можно придать функциональность, что при наведении на определённый объект, кисть автоматически меняет цвет или появляется контур (пример: навели на цветок – подсветился).

Выбор модели и реализация:

DeepLab vs SegFormer: DeepLabv3+ – классическая CNN модель сегментации. Она хорошо оптимизирована, CoreML-версия доступна, и на iOS может работать в реальном времени
it-jim.com
. Однако, её точность на некоторых классах ниже, плюс она достаточно тяжёлая (модель на COCO ~50 MB). SegFormer новее и в тестах более универсален, особенно вариант B0/B1.

Segment Anything (SAM): Отдельно, есть модель SAM от Meta, способная сегментировать любые объекты при подсветке курсором. Но она слишком тяжела для мобильного (image encoder – трансформер размером ViT-L/16). Также, SAM не классифицирует, а просто выделяет произвольные объекты – не то, что нам нужно прямо.

MediaPipe segmentation: Google имеет модуль MediaPipe для мобильной сегментации, например Selfie Segmentation
aihub.qualcomm.com
 (для отделения человека от фона). Но нам нужно не селфи, а общая сегментация. Возможно, можно использовать MediaPipe deeplab модель.

Оптимизация памяти: Интегрируя ML, нужно учесть память: на iOS CoreML модели загружаются в память, на Android – нужно загружать интерпретатор TFLite и модель в ByteBuffer. 15-20 MB модели – нормально, но помним, что ARKit/ARCore сами по себе используют RAM для карты мира, плюс камера поток.

Практические шаги:

Решить, нужна ли сегментация в MVP. Возможно, для первой версии достаточно возможностей ARKit классифицировать поверхности (на iOS) и просто не давать рисовать “в воздухе” (за пределами плоскостей) на Android. Интеграция ML имеет смысл, если мы хотим действительно улучшить взаимодействие (например, избегать объектов или позволить раскраску конкретных объектов по нажатию кнопки). Если в реальном времени сегментация не требуется, можно эту часть отложить, чтобы не усложнять сейчас.

Если принято решение внедрять:

Подготовить модель: взять SegFormer-B0 pre-trained (например, с HuggingFace или NVIDIA NGC). Проверить лицензию (Apache 2.0 – обычно открытая
aihub.qualcomm.com
). Конвертировать её: либо с помощью coremltools (для iOS), либо в ONNX/TFLite.

На iOS: создать VNCoreMLRequest или direkte MLModel inference. В Medium-статье показан пример, как разработчик использовал CoreML-модель DeepLab для сегментации в AR и преобразовал её вывод (513x513) в текстуру для occlusion
rozengain.medium.com
rozengain.medium.com
. Наш случай проще – можно получать результат как MultiArray или CVPixelBuffer, и проходиться по нему, выделяя нужные области.

На Android: интегрировать TFLite. Claude может помочь с генерацией кода загрузки модели (.tflite файл в assets) и выполнения inference в отдельно потоке. В результате будет, например, тензор с индексом класса для каждого пикселя исходного разрешения модели.

OpenCV: Альтернативно, если хотим действительно единообразно, можно создать c++ модуль с OpenCV DNN, который грузит ONNX и делает forward pass. Затем подключить его через JNI на Android и через Obj-C++ на iOS. Это сложнее, но даст один код. Claude может помочь с таким интеграционным кодом.

Использование на практике: Допустим, мы получили сегментацию кадра. Нужно интегрировать это с логикой рисования:

Например, запрет рисования на некоторых классах: если пиксель принадлежит классу "человек" или "объект" (не стена/пол), то при попытке рисования мы игнорируем этот ввод. Можно реализовать так: при каждом новом мазке проверять в карте сегментации ближайший пиксель – если класс не разрешён, не создавать мазок.

Или авто-выбор поверхности: например, настроить приложение в режим "раскрасить стены" – тогда автоматически все пиксели, которые модель считает стеной, можно закрасить выбранным цветом (например, фильтр).

Тестирование производительности: убедиться, что с запущенной моделью приложение не теряет в FPS слишком сильно. Если падает ниже, скажем, 15 FPS – нужно оптимизировать (квантовать модель, уменьшить частоту вызовов, использовать более мощное устройство для тестов и т.д.). SegFormer-B0, как отмечено, рассчитан на edge-девайсы
labellerr.com
, и Qualcomm оптимизировал его для Snapdragon (в Qualcomm AI Hub модель помечена для реалтайма на Snapdragon 8 Gen1+
aihub.qualcomm.com
).

В целом, интеграция ML – расширенная задача. Claude, выступая в роли помощника, может:

при необходимости написать код для запуска модели (Swift/CoreML или Java/TFLite),

подсказать, как постобрабатывать вывод,

и встроить это в основной цикл приложения (например, обновлять раз в N кадров, хранить текущую карту сегментации глобально).

Важно также отметить, что ARKit предоставляет определённые ML-функции “из коробки”:

People Occlusion: автоматическая сегментация людей на устройстве (Neural Engine) – достаточно выставить ARView.environment.sceneUnderstanding.options = .occlusion и люди будут маскироваться
stanford.edu
. Это покрывает сценарий не рисовать на людях.

Object Detection в Vision: можно обучить или использовать готовую MLModel для детекции конкретных объектов (например, распознавать мебель по фотографиям) – но это за пределами текущих нужд, да и SegFormer уже даёт классы мебели.

Подытоживая, ML-интеграция принесёт пользу, если мы хотим более «понимающее» AR-приложение, но она увеличит сложность. Решение о её необходимости может быть принято по ходу работы. Мы подготовили информацию, как это сделать, если потребуется.

4. Анализ и оптимизация архитектуры приложения

Прежде чем углубиться в кодинг новых фич, Claude должен проанализировать текущую архитектуру проекта и при необходимости предложить улучшения. Критерии оценки:

Модульность и разделение ответственности: Код должен быть организован так, чтобы логика AR, рендеринга, UI и ML были отделены. Например, можно применить MVC/MVVM:

Модель – данные сцены (меши, нарисованные объекты, возможно, результаты сегментации).

Вид – AR-окно, рендеринг графики.

Контроллер/ViewModel – менеджер, который получает вход от пользователя (тапы, жесты), дергает AR-функции (raycast, добавление anchors), обновляет модель и сообщает виду, что перерисовать.

Кросс-платформенность: Поскольку у нас две платформы, нужно минимизировать дублирование логики. Есть два основных подхода:

Раздельная нативная реализация: iOS и Android имеют свои проекты, но мы стараемся синхронизировать их поведение. Хорошая практика – описать общую логику на псевдокоде и имплементировать двумя способами. Можно также вынести общие части (например, алгоритмы обработки данных, ML inference) в библиотеку на C++ и подключить в оба (используя JNI и Obj-C++). Если используется OpenCV, это как раз пример общего C++ кода.

Единая движковая реализация (Unity): Рассмотреть использование Unity3D с AR Foundation – это позволило бы писать один код на C# для обоих платформ. AR Foundation поддерживает ARKit и ARCore под капотом. Например, он может автоматически использовать LiDAR-меши на поддерживаемых iOS устройствах
docs.unity3d.com
 и работать с плоскостями/глубиной на Android. Однако, AR Foundation не спасает от того факта, что ARCore не имеет meshing: Unity-проект тоже бы смог сканировать mesh только на iOS LiDAR
docs.unity3d.com
, а на Android ограничится плоскостями/окклюзией. Но единый код облегчает поддержку. Если большая часть работы уже сделана на Swift/Java, переключение на Unity сейчас может быть избыточным.

Если проект долгосрочный и команда владеет Unity, можно закладывать на будущее миграцию. Unity также даст готовые средства отрисовки, шейдеры для occlusion, и есть плагины для интеграции ML (Barracuda – фреймворк для запуска нейросетей в Unity). Но есть и overhead (приложение весит больше, нужны навыки Unity). Пока продолжим с нативным путем, но Claude может упомянуть Unity-специфичные моменты, если будет делать обзор.

Производительность (FPS): AR-приложение должно стремиться к 30 FPS (минимум) или 60 FPS (идеал на LiDAR-девайсах). Архитектурно, это значит:

Избегать блокирующих операций в основном потоке. Например, инференс ML или обработка меша – выполнять в бэкграунде (параллельный поток). На iOS можно использовать DispatchQueue.global() для ML, на Android – HandlerThread или Kotlin coroutines/Executors.

Постепенное обновление сцены: Сканирование mesh может порождать большое количество полигонов. Не обязательно сразу отрисовывать миллионы треугольников – можно отображать упрощенную версию или обновлять сетку частями.

Очистка памяти: Если сохраняем много anchor-объектов (мешей или рисованных объектов), убедиться, что при уходе со сцены они удаляются, иначе память будет утекать.

Реакция UI: Важно, чтобы интерфейс (например, нажатие кнопок смены цвета) не тормозил из-за AR. Разделение на UI-поток и поток рендеринга/AR должно быть чётким.

Расширяемость: Планируются ли дополнительные функции? Архитектура должна позволять добавить, например, сохранение нарисованного (persist anchors), совместную работу (multiuser AR), и т.п. Сейчас, возможно, нет, но хотя бы предусмотреть удобство: например, держать список нарисованных объектов, чтобы можно было сериализовать/десериализовать.

Архитектурный ревью кода: Claude должен просмотреть существующий код (если он будет ему предоставлен) и искать проблемные места. Типичные проблемы:

Смешение UI-кода и AR-кода (лучше их развести).

Использование Deprecated API (например, Sceneform на Android уже не поддерживается Google, хотя всё ещё работает, нужно ли переходить на другое?).

Отсутствие проверок доступности функций (например, не проверяется, есть ли LiDAR – и код крашится на iPhone без LiDAR).

Неоптимальные алгоритмы (скажем, каждый кадр происходит полная переработка всей mesh с нуля, вместо обновления инкрементально).

Документация и комментарии: В архитектурном смысле, убедиться, что сложные моменты (например, математика преобразования координат или интеграция ML) задокументированы. Claude при продолжении работы может добавлять комментарии к генерируемому коду для ясности.

Если архитектура признана неудовлетворительной, Claude может предложить рефакторинг. Например:

Вынести код рисования в отдельный DrawingManager класс, со статическим интерфейсом: startDrawing(color), addPoint(worldPosition), stopDrawing() – чтобы UI и ARSession вызывали его, а он сам следит, как генерировать мазки.

Сделать MeshManager для обработки приходящих ARMeshAnchor: он будет собирать mesh, объединять, предоставлять функции для перекрашивания (если надо).

Data models: Определить модели данных, например, PaintStroke (содержит список точек, цвет, толщина) – это удобно, если мы захотим перерисовать или пересохранить.

Platform Abstraction: Если очень хорошо продумать, можно сделать интерфейсы ARInterface с реализациями ARKitInterface и ARCoreInterface, скрывающие детали платформ. Тогда основной код (например, логика рисования) будет вызывать методы типа arInterface.raycast(screenPoint) и получать унифицированный результат. Claude может помочь сформировать такие абстракции. В простейшем случае, можно даже определить протокол/интерфейс in Swift/Kotlin, но проще – условная компиляция (если код раздельно пишется).

В целом, этап архитектурного анализа важно выполнить перед кодированием, чтобы новые функции “ложились” на прочный фундамент. Claude должен:

перечислить выявленные проблемы (если исходный код будет виден),

предложить решение (рефакторинг, шаблон проектирования),

согласовать с желаемым функционалом.

5. Продолжение реализации и оптимизация

После того, как архитектура проверена/улучшена, Claude приступит непосредственно к реализации функциональности:

Написание кода для AR-мешинга на iOS: настройка ARSession, обработка ARMeshAnchor. Возможно, предоставление пользователю визуализации меша (например, переключатель “показать сетку”).

Код для Android ARCore: обновление текущего кода обнаружения плоскостей, использование Depth API. Claude может генерировать фрагменты на Java/Kotlin, интегрирующие Depth API (что включает запрос разрешения на Depth, использование acquireDepthImage() из кадра и т.д.).

Реализация рисования: код обработки жестов (Touch events in ARView for iOS, onTouchEvent for Android’s GLSurfaceView или SceneView). Создание объектов или текстур. Тут будет много платформенного кода – Claude сможет это делать поочерёдно: сначала, например, реализовать на iOS (Swift + RealityKit/SceneKit), затем аналог на Android.

Интеграция ML (если решили внедрять на этом этапе): Генерация кода для загрузки модели, выполнение её при кадрировании. Например, Claude может написать функцию, которая берёт CVPixelBuffer из ARFrame (iOS) и прогоняет через CoreML модель, возвращая VNPixelBufferObservation с маской, затем накладывает её. Или на Android – как загрузить TFLite модель из assets и получить вывод. При этом он будет ссылаться на известные библиотеки (например, org.tensorflow.lite.Interpreter).

Тестирование и отладка: Claude должен помогать писать тестовые сценарии: например, проверить, что рисование работает под разными углами камеры, что на iOS на устройстве без LiDAR не происходит ошибок, что на Android при движении камера точки рисования остаются на правильном месте (не дрейфуют – если трекинг теряется, возможно, стоит привязывать мазки к якорям).

Оптимизация рендеринга: Если будет видно, что, например, при большом количестве нарисованных объектов fps проседает, можно применить батчинг. Claude может предложить: “Соединять близко расположенные кружки в одну mesh” или “ограничить максимальное число мазков”.

Память: убедиться, что нет утечек – например, на Android освобождать Image из depthImage, на iOS не хранить ARAnchor, если они уже не актуальны.

Каждый шаг реализации желательно снабжать комментариями и ссылками на документацию (для обоснования используемых методов). Например, если Claude пишет ARSession.raycast – он может сослаться на Apple Doc. Если использует Session.acquireDepthImage() – на Google документацию.

Интеграция OpenCV

Кроме ML, у нас прямое упоминание OpenCV – возможно, для каких-то CV-задач. Если не использовать OpenCV DNN, всё же OpenCV может пригодиться для:

Постобработки изображений (например, фильтрации карты сегментации, сглаживания краёв).

Конвертации между форматами изображений.

Возможно, для собственных простых эффектов (например, сделать размытие краёв нарисованной линии).
Claude может задействовать OpenCV C++ через JNI если это требуемая часть (но это большой кусок работы сам по себе, возможно, не стоит если TFLite/CoreML справляются).

6. План действий (резюме)

Ниже приводится пошаговый план, который Claude (как AI-помощник) должен выполнить, помогая в разработке:

Анализ текущего кода и архитектуры: Просмотреть существующий код базы (модули iOS и Android) – выявить места, требующие изменений для поддержки meshing и рисования. Проверить, как сейчас организовано обновление сцены, есть ли placeholders для рисования. Результат: список предложений по рефакторингу и подтверждение подхода (нативно vs Unity).

Реализация сканирования окружения (meshing/planes):

Для iOS: включить sceneReconstruction, обрабатывать ARMeshAnchor в сессии. Визуализировать mesh (например, через добавление ModelEntity с созданной геометрией). Убедиться, что классификации доступны, и при желании – раскрасить mesh по типам поверхностей (в отладочных целях, либо для интереса пользователя)
docs.unity3d.com
.

Для Android: включить получение глубины (если поддерживается), реализовать получение плоскостей. (Опционально: визуализировать точки глубины – например, рисовать маленькие точки по облаку для эффекта “точечного мезха”). Проверить, что на поддерживаемых устройствах (пример: Pixel 4+ или Samsung S20+) Depth API работает.

Обработка несовместимых устройств: на iOS без LiDAR – либо сообщить, что 3D-скан недоступен, либо ограничиться плоскостями (ARKit PlaneDetection). На Android без Depth (или без ARCore вообще) – вывести сообщение об отсутствии поддержки.

Реализация механики рисования:

Разработать систему ввода: отслеживание касания/жеста. Для простоты: при касании экрана сразу создавать мазок, при движении – продолжать добавлять сегменты.

Создавать виртуальные объекты мазков: например, маленькие сферические маркеры или плоские круги. Настроить их материал (цвет, возможно, небольшой эффект прозрачности).

Реализовать разные режимы: рисование одной непрерывной линией (drag) vs рисование точками (tap). Также предусмотреть стирание (если нужно – например, луч прожектора для удаления нарисованного).

Внедрить UI-кнопки: выбор цвета, очистка рисунка, переключение между режимами (если планируется).

Протестировать на разных поверхностях: рисование на горизонтальной плоскости (пол) – мазки должны лежать ровно; рисование на вертикали – мазки висят на стене; если рисовать “в воздухе” (куда луч не попал) – можно либо не создавать мазок, либо создавать в той последней точке, где был контакт с поверхностью.

Интеграция ML (если применимо на этом этапе):

Подготовить инфраструктуру вызова модели: загрузка модели при запуске приложения (или при входе в режим, требующий модели).

Сделать ассинхронный вызов inference на кадре (можно взять кадр камеры в низком разрешении, прогнать через сегментацию).

Получить выход – например, матрицу классов – и использовать его для ограничения рисования. Тестовый сценарий: попробуйте рисовать на объекте (например, стуле) – если сегментация видит “стул”, приложение не должно рисовать “за” стул (или должно выдать подсказку).

Если интеграция ML не нужна немедленно, убедиться, что архитектура позволяет добавить её позднее без больших изменений (например, заложить интерфейс SceneUnderstanding класс, который может внутри либо звать ARKit classification, либо ML-модель – и пока вернуть заглушки).

Тестирование и отладка на устройствах:

Прогнать приложение на реальном iPhone/iPad с LiDAR: проверить сканирование (получается ли сетка, правильны ли масштабы), проверить рисование (нет ли лагов при добавлении объектов, корректно ли работает occlusion – на iOS должно сразу работать хорошо, особенно с LiDAR-мешем).

На Android устройстве с поддержкой Depth: проверить, что occlusionManager (при наличии, например, в ARCore-Extensions) скрывает виртуальные мазки за реальными объектами. Если нет – проверить наш способ occlusion (через шейдер глубины).

Проверить, что UI отзывчив: смена цвета происходит мгновенно, нажатия не задерживаются из-за фоновых вычислений.

Измерить примерный FPS: вручную или с помощью профайлера. Если FPS проседает значительно при включенном mesh или при рисовании сотни объектов, отметить это.

Оптимизация (при необходимости):

Если mesh слишком детализирован (например, LiDAR генерирует очень много треугольников), можно снизить детализацию: ARMeshAnchor имеет параметры, возможно, можно настроить плотность (в ARWorldTrackingConfiguration можно задать .init(sceneReconstruction: .mesh, detectionMode: .continuous) – не уверен, можно ли регулировать).

Реализовать pooling для объектов рисования: вместо создания тысячи отдельных узлов, переиспользовать или объединять их. Например, каждые N точек объединять в один MeshGeometry (Claude может рассчитать вершины для кружочков и объединить).

Память: большие структуры (mesh, depth maps, ML buffers) – освобождать, когда не нужны. ARKit сам убирает старые anchors по мере ухода из поля зрения, но можно вручную удалять далёкие или мелкие элементы.

ML оптимизация: если модель загружена и мешает, выгружать её, когда не нужна (в CoreML выгрузить сложно, но можно удалить ссылки; в TFLite – занулить Interpreter).

Документация и комментарии: Добавить описание в код, как всё работает, чтобы команде было понятно и для будущей поддержки. Также, возможно, подготовить README (кратко) для репозитория, описывающее как запускать на разных платформах, какие требования (LiDAR, список поддерживаемых Android устройств, etc).

Заключение

В данной инструкции собраны все ключевые аспекты для продолжения разработки AR-приложения с функцией сканирования меша и рисования на поверхностях, включая возможную интеграцию машинного зрения. Приведены ссылки на официальную документацию и исследования для обоснования подходов:

ARKit предоставляет продвинутые возможности (LiDAR-меш, классификация поверхностей)
docs.unity3d.com
docs.unity3d.com
, которые мы будем использовать на iOS.

ARCore пока ограничен в meshing, но мы используем его Depth API и Plane detection
codelabs.developers.google.com
, сохраняя сопоставимый функционал на Android.

Для улучшения понимания сцены рассмотрена модель SegFormer – лёгкая и быстрая для мобильных устройств
labellerr.com
aihub.qualcomm.com
, а также альтернативы (Core ML, TFLite) для её запуска.

Обозначены шаги по реализации интерактивного рисования, с учётом технических деталей (raycast, проекция, визуализация) и пользовательского опыта.

Особое внимание уделено архитектуре и производительности, чтобы приложение было устойчивым и плавным. Советы опираются на лучшие практики мобильной разработки и особенности AR SDK (например, use of scene depth for occlusion
docs.unity3d.com
).

Следующий шаг: Claude, получив эту инструкцию, должен приступить к выполнению плана. Сначала – произвести оценку текущего состояния кода (возможно, пользователь предоставит фрагменты или описания). Затем, шаг за шагом, реализовывать вышеописанные функции, проверяя корректность на каждом этапе.

Claude должен действовать последовательно, не упуская ни одного пункта, поясняя свои действия по необходимости и сохраняя ссылки на источники при обосновании технических решений. Такой тщательный, планомерный подход позволит успешно расширить функциональность приложения до требуемого уровня.