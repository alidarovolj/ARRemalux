Стратегический обзор и дорожная карта: от `` к семантическому AR с Core MLИСПОЛНИТЕЛЬНОЕ РЕЗЮМЕ:Анализ предоставленных логов, кода и изображений выявил две отдельные проблемы.Критическая (Уровень 1): Непосредственная проблема заключается в ошибке конфигурации Unity, а не в коде. Значения ``, установленные в Inspector, переопределяют значения по умолчанию, заданные в C#-скрипте. Это приводит к тому, что сборка использует устаревшие, жесткие параметры фильтрации (например, `$minWallHeight: 0,8$ м), несмотря на изменения в коде. Эта проблема решается немедленно путем сброса компонента в Inspector.Архитектурная (Уровень 10): Устранение первой проблемы выявит фундаментальный недостаток текущего подхода. Система ARPlaneDetection (геометрический поиск) по своей сути неспособна к семантическому пониманию. Она не может отличить "стену" от "телевизора" (Image 2) или "пола". Кроме того, она не может обрабатывать динамические окклюзии, такие как "кот" (Image 1), и плохо работает на гладких поверхностях.В данном отчете сначала представлен немедленный протокол устранения проблемы ``. Затем приводится критический анализ архитектурных ограничений. Наконец, в нем излагается подробная дорожная карта для миграции вашего приложения от базового геометрического AR к продвинутому семантическому AR с использованием ARKit Mesh Classification, Apple RoomPlan и пользовательских моделей Core ML для достижения высочайшего качества работы.Часть 1: Немедленная диагностика и устранение: Критическая проблема c ``Ваша основная проблема — это не ошибка сборки или кэша, а фундаментальный принцип работы системы сериализации Unity.Раздел 1.1: Диагноз: Приоритет сериализации в UnityВы столкнулись с классическим поведением Unity. Атрибут `` (или любое public поле) указывает Unity сохранить это значение как часть сцены или префаба.1Процесс происходит следующим образом:Первоначальное сохранение: Когда вы впервые добавили скрипт WallDetectionAndPainting на GameObject в вашей сцене, в коде были старые значения (например, $minWallHeight = 0.8f$). Unity считала это значение по умолчанию и *сериализовала* (сохранила) его в сам файл сцены (.unity`).3Изменение кода: Позже вы изменили значение по умолчанию в коде (на $minWallHeight = 0.2f$).Конфликт приоритетов: При загрузке сцены (или при сборке) Unity видит два значения для minWallHeight:Значение по умолчанию из C# кода: $0.2f$Сериализованное значение из .unity файла: $0.8f$Unity всегда отдает приоритет значению, сохраненному в сцене (Inspector).5 Это сделано намеренно, чтобы дизайнеры уровней могли настраивать параметры в Inspector, и их работа не "сбрасывалась" каждый раз, когда программист меняет значение по умолчанию в коде.Ваш код не "игнорируется"; он просто имеет более низкий приоритет, чем данные, уже "запеченные" в вашу сцену.Раздел 1.2: Протокол немедленного исправленияЧтобы заставить Unity использовать новые значения по умолчанию из кода, вы должны удалить переопределенные значения из сцены. Самый быстрый способ — это Reset.Шаг 1: Поиск GameObject (Ответ на Вопрос 6)Компонент, скорее всего, находится на вашем главном AR-объекте (XR Origin, AR Session Origin или Main Camera). Чтобы найти его мгновенно:В окне Hierarchy в Unity используйте строку поиска.Введите имя скрипта: WallDetectionAndPainting.8Экспертный метод: Введите t:WallDetectionAndPainting. Это синтаксис поиска по типу, который немедленно отфильтрует иерархию, показав только те GameObject, которые содержат этот компонент.10Шаг 2: Сброс значений (Ответ на Вопрос 3)Выберите найденный GameObject.В окне Inspector найдите компонент Wall Detection And Painting.Нажмите на три точки (⋮) в правом верхнем уггу этого компонента.Выберите "Reset".12После этого вы мгновенно увидите, как значения в Inspector изменятся с $0.8$, $0.5$ и т.д. на $0.2$, $0.1$ из вашего C# кода.Шаг 3: СохранениеНажмите Ctrl+S (или File > Save Scene), чтобы сохранить эти изменения в .unity файле. Теперь новые значения являются сохраненными.Раздел 1.3: Протокол верификации и сборки (Ответ на Вопросы 4, 7)Верификация до сборки (Ответ на Вопрос 7):Визуально: Просто посмотрите в Inspector. Если там $0.2$, все в порядке.Программно: Добавьте Debug.Log в метод Awake() вашего скрипта. Awake() выполняется после применения сериализованных значений, поэтому он покажет то, что реально будет использоваться.C#void Awake() {
    Debug.Log($" ФИЛЬТРЫ ЗАГРУЖЕНЫ: minWallHeight = {minWallHeight}");
}
Запустите сцену в редакторе (Play Mode) и посмотрите в Console.15Режим Debug: Нажмите на три точки (⋮) в верхнем правом углу всего окна Inspector (не компонента) и переключите его из "Normal" в "Debug". Это показывает "сырые" данные объекта и подтверждает, что нет скрытых переопределений.16Гарантии сборки (Ответ на Вопрос 4):Ваш процесс сборки должен быть следующим, чтобы гарантировать отсутствие "устаревших" данных:Unity: File > Build Settings > Build. Выберите папку для сборки и используйте опцию "Replace". Этого обычно достаточно со стороны Unity.18 Вам не нужно Reimport All, так как это не связано с импортом ассетов.19Xcode: Откройте проект в Xcode.Xcode: Выполните Product > Clean Build Folder (Ярлык: Shift+Cmd+K). Этот шаг критически важен и часто пропускается. Он удаляет старые скомпилированные артефакты и заставляет Xcode заново собрать проект с чистыми данными, полученными от Unity.21Xcode: Запустите (Run) приложение на устройстве.Часть 2: Критический анализ текущего подхода: Архитектурный тупикУстранение бага `` (Часть 1) не решит вашу проблему. Оно лишь выявит более глубокие недостатки вашего текущего подхода, основанного только на ARPlaneDetection.Раздел 2.1: Почему исправленные фильтры не работаютКак только вы примените новые, "мягкие" фильтры (например, `$minCenterHeightY = 0.1$ или даже $-0.5$), вы столкнетесь с обратной проблемой: ложноположительные срабатывания.Анализ лога centerY=-0.11м:centerY в ARKit — это позиция центра плоскости относительно точки старта AR-сессии (т.е. высоты камеры при запуске).Значение $centerY=-0.11$ означает, что центр найденной плоскости находится на 11 см ниже точки, где вы держали телефон.Это почти наверняка пол, а не стена.Ваш предложенный фильтр minCenterHeightY: -0.5м позволит приложению принять пол за стену. Вы пытаетесь решить проблему отсутствия детекции, создавая проблему ложной детекции.Проблема гладких стен (Image 1, Image 2):ARPlaneDetection — это геометрический инструмент. Он работает путем поиска контрастных признаков (feature points).22 Стены на ваших фотографиях — гладкие, однотонные, с блестками, что является худшим сценарием для этой технологии.23 ARKit будет либо не находить их, либо найденные плоскости будут "дребезжать", иметь неверную форму и появляться с большой задержкой.Раздел 2.2: Юнит-тесты из реального мира: Телевизор (Image 2) и Кот (Image 1)Ваши собственные фотографии демонстрируют два фатальных недостатка текущей архитектуры.1. Статическая окклюзия (Image 2: Телевизор):ARPlaneDetection — это геометрия. Он не знает, что он нашел. Ваш телевизор на стене — это идеальная вертикальная плоскость с точки зрения геометрии. Ваше приложение с новыми фильтрами обнаружит телевизор и предложит пользователю его покрасить.26 Это недопустимый пользовательский опыт.2. Динамическая окклюзия (Image 1: Кот):Кот, спящий на полу, представляет вторую проблему. Что если он подойдет к стене, когда пользователь начнет красить? Что если ребенок или другой член семьи войдет в кадр?Ваш текущий подход не имеет механизма для окклюзии (occlusion). Виртуальная краска будет рендериться поверх кота, человека и игрушек, полностью разрушая эффект дополненной реальности.Вывод: Ваша проблема не в фильтрах. Ваша проблема в инструменте. ARPlaneDetection не подходит для этой задачи. Вам необходим переход от геометрии ("найти плоскую штуку") к семантике ("найти стену, но не телевизор, и не кота").Часть 3: Дорожная карта по внедрению Machine Learning для семантического ARВы абсолютно правы, что смотрите в сторону ML. Это и есть решение. Однако стратегия интеграции ML нетривиальна.Раздел 3.1: Фундаментальный сдвиг парадигмы: ML не для поиска стенПервым шагом был анализ моделей семантической сегментации, на которые вы ссылаетесь, таких как DeepLabv3 и DETR Resnet50 Semantic Segmentation.27 Затем был проведен анализ стандартных наборов данных, на которых обучаются эти модели, в первую очередь COCO и PASCAL VOC.27Этот анализ выявил критически важный факт:Популярные модели семантической сегментации не обучены распознавать "Стену" (wall) как класс.Списки классов COCO и PASCAL содержат: person, cat, dog, tv (или tvmonitor), couch (или sofa), chair, bed.31Они не содержат: wall, floor, ceiling.Это полностью меняет архитектуру. Вы не должны использовать ML для поиска стен.Новая архитектура: Используйте ARKit для поиска геометрии (стены, пол) и Core ML для поиска семантики (окклюзий).Ваша логика должна быть:Окрашиваемая_Зона = Геометрия_Стены - Семантическая_Маска_ОкклюзийML решит ваши проблемы с Image 1 (кот) и Image 2 (телевизор), создав 2D-маску для tv и cat, и вы вычтете эту маску из зоны покраски.Раздел 3.2: Путь 1 (Хорошо): Переход на ARMeshAnchor и классификацию (Требует LiDAR)Если ваш "iPhone с A15 chip" — это iPhone 13 Pro/14 Pro/15 Pro, у вас есть LiDAR. Это самый простой и мощный апгрейд.Прекратите использовать ARPlaneDetection.Переключите вашу AR-конфигурацию на использование Scene Geometry (ARMeshAnchor).38 В AR Foundation это делается через ARMeshManager.Что это дает: LiDAR мгновенно сканирует комнату, включая гладкие стены, и создает 3D-меш. Проблема детекции гладких стен решается.39Встроенная семантика: AR Foundation 5+ предоставляет ARMeshClassification.38 Каждый ARMeshAnchor содержит данные, которые уже классифицируют полигоны меша как ARMeshClassification.Wall, .Floor, .Ceiling, .Furniture, .Window и т.д.Решение: Ваша логика покраски упрощается до: "разрешить покраску только на полигонах, где классификация равна .Wall". Это автоматически отфильтрует пол, потолок и телевизор (который, скорее всего, будет помечен как .Furniture).38Раздел 3.3: Путь 2 (Лучше): Интеграция Core ML для семантических окклюзийЭто необходимо, если:Вы не можете требовать LiDAR (вы целитесь в базовые iPhone без Pro).Вам нужна динамическая окклюзия для объектов, которые не знает ARMeshClassification (например, ваш кот из Image 1).Задача: Взять модель типа DeepLabv3.mlmodel 27, обученную на PASCAL/COCO, и в реальном времени получать 2D-маску сегментации для person, cat, tv и т.д.Вариант А: "Кроссплатформенный" — Unity Sentis (Barracuda)Процесс: Вы берете .mlmodel (или модель TensorFlow/PyTorch) и конвертируете ее в формат .onnx.40 Затем вы используете Unity-пакет com.unity.sentis (ранее Barracuda) для загрузки и запуска этой .onnx модели в C#.41Плюсы: Все пишется на C#. Теоретически кроссплатформенно.Минусы: Конвертация моделей может быть сложной. Выполнение нейросети "поверх" Unity менее эффективно, чем нативный запуск на Neural Engine.Вариант Б: "Нативный" — Swift-плагин (Рекомендуется для iOS)Процесс: Вы создаете нативный плагин.44 Это мост: C# скрипт в Unity вызывает Objective-C, который вызывает Swift-файл.45Swift-код: Ваш Swift-код получает кадр камеры (CVPixelBuffer) из session.currentFrame.capturedImage.46 Он использует фреймворк Vision для создания VNCoreMLRequest с вашей моделью DeepLabv3.mlmodel.47Результат: Vision чрезвычайно быстро (на Neural Engine) выполняет сегментацию и возвращает 2D-маску.49 Эта маска (как массив байт) передается обратно в Unity.Плюсы: Максимально возможная производительность на устройстве.Минусы: Высокая сложность первоначальной настройки "моста" C#-Swift.44Проекция 2D-маски на 3D-геометрию:Получив 2D-маску (cat, tv), вы не можете просто нарисовать ее поверх экрана. Вам нужен кастомный шейдер для вашей "краски". Шейдер должен будет "поднимать" (project) 2D-пиксели маски на 3D-геометрию стены.50 Если пиксель на маске соответствует tv или cat, шейдер отменяет отрисовку краски в этой точке (используя discard или alpha). Это продвинутая техника, но именно так работает кастомная AR-окклюзия.52Часть 4: Решение «Золотого стандарта»: Интеграция API RoomPlanСуществует третий, самый продвинутый путь, который Apple создала специально для таких задач, как ваша.Раздел 4.1: Что такое RoomPlanRoomPlan — это не ARKit, а отдельный Swift-фреймворк.54Что он делает: Используя камеру и LiDAR, он проводит сканирование комнаты.54Результат: Он создает высокоточную, параметрическую 3D-модель комнаты.56 Он не просто дает меш; он выводит USDZ-файл, в котором семантически размечены wall, door, window, opening и furniture (включая television, couch и т.д.).56Решение: RoomPlan идеально решает вашу задачу со статической средой. Он:Надежно находит гладкие стены (благодаря LiDAR).56Семантически отделяет wall от furniture (телевизора).56Дает вам чистую 3D-модель только стен, на которой можно рисовать.26Раздел 4.2: Стратегия интеграции RoomPlan в UnityОграничения:Требует LiDAR (iPhone Pro).54Это Swift API, оно не включено в AR Foundation по умолчанию.55Путь А (Сложный): Создать нативный плагин (как в 3.3, Вариант Б) для вызова RoomCaptureSession и RoomBuilder.58 Вы будете получать CapturedRoom объект, экспортировать его в USDZ и загружать этот меш в Unity.Путь Б (Рекомендуемый): Купить готовый плагин в Unity Asset Store. Существуют проверенные ассеты, такие как "RoomPlan for Unity Kit" 60, которые уже предоставляют C#-обертки для нативного API RoomPlan и поддержку AR Foundation. Это сэкономит месяцы работы.Раздел 4.3: Гибридная архитектура "State-of-the-Art" (SOTA)RoomPlan великолепен для статической сцены (стены, ТВ). Но он сканирует комнату один раз.58 Он не узнает, если ваш кот (Image 1) войдет в комнату после сканирования.Идеальная архитектура — гибридная:Фаза 1 (Сканирование): Используйте RoomPlan (через плагин 62) для получения идеальной, семантически чистой 3D-модели стен и мебели.Фаза 2 (Окраска): Отобразите этот 3D-меш в AR-сессии. Позвольте пользователю рисовать только на меше с тегом wall.Фаза 3 (Окклюзия): Одновременно с этим используйте встроенную в ARKit 5+ функцию Person Segmentation (AROcclusionManager в Unity) 53 для окклюзии людьми и (если кот важен) кастомную модель Core ML (из Раздела 3.3) для окклюзии животных.Этот подход решает все ваши проблемы: надежное детектирование стен, игнорирование статической мебели (ТВ) и окклюзия динамическими объектами (люди, кот).Часть 5: Итоговая стратегия и рекомендуемые дальнейшие шагиРаздел 5.1: Сводный план действийНемедленно (Сегодня):Найдите GameObject с вашим скриптом, используя поиск в иерархии t:WallDetectionAndPainting.11Нажмите Reset Component (⋮).12Сохраните сцену (Ctrl+S).В Xcode сделайте Clean Build Folder (Shift+Cmd+K) 21 и запустите.Результат: Ваш баг с `` будет исправлен.Краткосрочно (Эта неделя) — "Хорошее" решение (Требует LiDAR):Удалите ARPlaneManager.Добавьте ARMeshManager.38Включите ARMeshClassification.38Измените логику покраски, чтобы она работала только с полигонами, которые ARMeshClassification помечает как Wall.Результат: Приложение будет надежно находить гладкие стены и в основном игнорировать телевизор.Среднесрочно (Следующий месяц) — "Лучшее" решение (Требует LiDAR):Приобретите и интегрируйте плагин "RoomPlan for Unity Kit".62Перестройте приложение на двухфазный режим: "Сначала сканируй (RoomPlan)", затем "Рисуй (AR)".Используйте сгенерированный RoomPlan USDZ-меш в качестве целевой поверхности для рисования.56Результат: Семантически идеальная 3D-модель комнаты. Покраска телевизора становится невозможной.Долгосрочно (Этот квартал) — "Идеальное" (SOTA) решение:К решению из шага 3 добавьте динамическую окклюзию.Включите AROcclusionManager в AR Foundation на personSegmentation.53Если нужна окклюзия животных: создайте нативный Swift-плагин (Раздел 3.3, Вариант Б) для запуска модели DeepLabv3 27 и создания 2D-маски для cat 31, которую вы будете использовать в кастомном шейдере окклюзии.Результат: Приложение мирового класса, которое корректно обрабатывает статическую (ТВ) и динамическую (люди, кот) среду.Раздел 5.2: Сравнительный анализ архитектур AR-детектированияЭта таблица суммирует варианты для принятия стратегического решения.АрхитектураТочность на гладких стенахРаспознавание окклюзий (ТВ/Мебель)Распознавание (Кот/Человек)Требование LiDARСложность интеграции1. ARPlaneDetection (Ваш подход)❌ Очень низкая [22, 23]❌ Нет (Детектирует ТВ как стену) 26❌ НетНетНизкая2. ARMeshAnchor + Classification✅ Высокая✅ Да (Классифицирует как Furniture) 38❌ Нет (Человек - отдельно)Да 38Средняя (встроено в ARF)3. Custom Core ML (DeepLabv3)❌ (Зависит от ARPlaneDetection)✅ Да (Сегментирует tv, sofa) 31✅ Да (Сегментирует cat, person) [31]Нет⚠️ Очень высокая [44, 46]4. RoomPlan API (Плагин)✅ Идеальная 56✅ Идеальная (Семантическая модель) 56❌ Нет (Только статика) 58Да [54, 57]Средняя (с плагином 62)5. RoomPlan + Core ML (Гибрид SOTA)✅ Идеальная✅ Идеальная (статика)✅ Да (динамика) [53]Да⚠️ Очень высокая